pixel_data <- image_data(img)
# convert rgb values to 0-255
image_values <- as.integer(pixel_data)
# create an empty dataframe
image_df <- data.frame(barcode = numeric(nrow(location)),
x = numeric(nrow(location)),
y = numeric(nrow(location)),
R = numeric(nrow(location)),
G = numeric(nrow(location)),
B = numeric(nrow(location)),
RS = numeric(nrow(location)),
GS = numeric(nrow(location)),
BS = numeric(nrow(location))
)
max_x = max(location$x)
max_y =max(location$y)
radius = r
for(i in 1:nrow(location)){
image_df$barcode[i] = row.names(location)[i]
image_df$y[i] = location$y[i]
image_df$x[i] = location$x[i]
# original rgb values
image_df$R[i] = image_values[location$y[i],location$x[i],1]
image_df$G[i] = image_values[location$y[i],location$x[i],2]
image_df$B[i] = image_values[location$y[i],location$x[i],3]
# take average rgb values around the spot
image_df$RS[i] <- round(mean(image_values[max(0,location$y[i]-radius):min(max_y,location$y[i]+radius),
max(0,location$x[i]-radius):min(max_x,location$x[i]+radius),
1]))
image_df$GS[i] <- round(mean(image_values[max(0,location$y[i]-radius):min(max_y,location$y[i]+radius),
max(0,location$x[i]-radius):min(max_x,location$x[i]+radius),
2]))
image_df$BS[i] <- round(mean(image_values[max(0,location$y[i]-radius):min(max_y,location$y[i]+radius),
max(0,location$x[i]-radius):min(max_x,location$x[i]+radius),
3]))
}
image_df$grey <- image_df$RS*0.299+
image_df$GS*0.587+
image_df$BS*0.114
object <- new(
Class = "TUSCAN",
counts.data = counts,
project = project,
location = location,
image = image_df
)
return(object)
}
B1_obj <- createTUSCANObject(counts=B1_mat, location = B1_loc, img = img_B1)
reduceDim <- function(object,nhvg=2000,d=20,epochs=50,seed){
Seurat_obj <- CreateSeuratObject(counts = object@counts.data, min.cells = 0,
min.genes = 0, project = "TUSCAN")
Seurat_obj <- NormalizeData(Seurat_obj, normalization.method = "LogNormalize", scale.factor = 10000)
hvgs <- FindVariableFeatures(Seurat_obj, selection.method = "vst", nfeatures = 2000)
# Identify the 2000 most highly variable genes (hvgs)
top2k <- VariableFeatures(hvgs)
####### Pre-clustering #########
### clustering based on autoencoder + scaled RGB
##  2k hvgs -> autoencoder -> N outputs + rescaled RGB
log_mat <- Matrix::t(Seurat_obj@assays$RNA@data)
hvg_mat <- log_mat[,top2k]
# autoencoder
print("Start dimension reduction by autoencoder...")
tensorflow::set_random_seed(seed)
features_matrix <- Matrix::as.matrix(hvg_mat)
input_dim <- nhvg
encoding_dim <- d # dim after dimension decrease
input_layer <- layer_input(input_dim)
# set different number of hidden layers
encoder_layer <-
input_layer %>%
layer_dense(units = 1500, activation = "relu", kernel_regularizer = regularizer_l1(0.0001)) %>%
layer_batch_normalization() %>%
layer_dense(units = 1000, activation = "relu", kernel_regularizer = regularizer_l1(0.0001)) %>%
layer_dense(units = 500, activation = "relu") %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dense(units = encoding_dim) # 2 dimensions for the output layer
decoder_layer <-
encoder_layer %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dense(units = 500, activation = "relu") %>%
layer_dense(units = 1000, activation = "relu") %>%
layer_dense(units = 1500, activation = "relu") %>%
layer_dense(units = input_dim)
autoencoder_model <- keras_model(inputs = input_layer, outputs = decoder_layer)
autoencoder_model %>% compile(
loss='mean_squared_error',
optimizer='adam',
metrics = c('accuracy')
)
set.seed(seed)
autoencoder_model %>%
keras::fit(features_matrix,
features_matrix,
epochs=epochs,
shuffle=TRUE,
verbose=0
)
encoder <- keras_model(inputs = autoencoder_model$input, outputs = encoder_layer)
encoded_data <- predict(encoder, features_matrix)
object@autoencoder <- encoded_data
print("Dimension reduction finished...")
return(object)
}
PreCluster <- function(object,res=NULL,seed){
img <- object@image
encoded_data <- object@autoencoder
rescale_to_range <- function(x,gmin,gmax){
rescaled_x <- (x-min(x))/(max(x)-min(x))*(gmax-gmin)+gmin
return(rescaled_x)
}
global_min <- mean(apply(encoded_data, MARGIN = 2, FUN = min))
global_max <- mean(apply(encoded_data, MARGIN = 2, FUN = max))
img <- img %>% mutate(RS_r=rescale_to_range(img$RS,global_min,global_max),
GS_r=rescale_to_range(img$GS,global_min,global_max),
BS_r=rescale_to_range(img$BS,global_min,global_max))
features <- cbind(encoded_data,img[,c("RS_r","GS_r","BS_r")])
# louvain clustering
set.seed(seed)
info.spatial = as.data.frame(features)
colnames(info.spatial) =  paste0("factor", 1:ncol(features))
knn.norm = FNN::get.knn(as.matrix(features), k = 100)
knn.norm = data.frame(from = rep(1:nrow(knn.norm$nn.index),
k=100), to = as.vector(knn.norm$nn.index), weight = 1/(1 + as.vector(knn.norm$nn.dist)))
nw.norm = igraph::graph_from_data_frame(knn.norm, directed = FALSE)
nw.norm = igraph::simplify(nw.norm)
if(is.null(res)){
lc.norm = igraph::cluster_louvain(nw.norm)
}else{
lc.norm = igraph::cluster_louvain(nw.norm,resolution = res)
}
print(paste("Found", length(unique(membership(lc.norm))), "clusters"))
img$cluster <- factor(lc.norm$membership)
object@pre_cluster <- factor(lc.norm$membership)
object@image <- img
# save clustering results
df <- data.frame(barcode=img$barcode, cluster=object@pre_cluster)
row.names(df) <- df$barcode
object@annotation <- df
write.table(df, file = "annotations.txt", quote = FALSE, col.names = FALSE, row.names = FALSE, sep = "\t")
return(object)
}
B1_obj2 <- reduceDim(object = B1_obj,seed=123)
reduceDim <- function(object,nhvg=2000,d=20,epochs=50,seed){
Seurat_obj <- CreateSeuratObject(counts = object@counts.data, min.cells = 0,
min.genes = 0, project = "TUSCAN")
Seurat_obj <- NormalizeData(Seurat_obj, normalization.method = "LogNormalize", scale.factor = 10000)
hvgs <- FindVariableFeatures(Seurat_obj, selection.method = "vst", nfeatures = 2000)
# Identify the 2000 most highly variable genes (hvgs)
top2k <- VariableFeatures(hvgs)
####### Pre-clustering #########
### clustering based on autoencoder + scaled RGB
##  2k hvgs -> autoencoder -> N outputs + rescaled RGB
log_mat <- Matrix::t(Seurat_obj@assays$RNA@data)
hvg_mat <- log_mat[,top2k]
# autoencoder
print("Start dimension reduction by autoencoder...")
#tensorflow::set_random_seed(seed)
features_matrix <- Matrix::as.matrix(hvg_mat)
input_dim <- nhvg
encoding_dim <- d # dim after dimension decrease
input_layer <- layer_input(input_dim)
# set different number of hidden layers
encoder_layer <-
input_layer %>%
layer_dense(units = 1500, activation = "relu", kernel_regularizer = regularizer_l1(0.0001)) %>%
layer_batch_normalization() %>%
layer_dense(units = 1000, activation = "relu", kernel_regularizer = regularizer_l1(0.0001)) %>%
layer_dense(units = 500, activation = "relu") %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dense(units = encoding_dim) # 2 dimensions for the output layer
decoder_layer <-
encoder_layer %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dense(units = 500, activation = "relu") %>%
layer_dense(units = 1000, activation = "relu") %>%
layer_dense(units = 1500, activation = "relu") %>%
layer_dense(units = input_dim)
autoencoder_model <- keras_model(inputs = input_layer, outputs = decoder_layer)
autoencoder_model %>% compile(
loss='mean_squared_error',
optimizer='adam',
metrics = c('accuracy')
)
set.seed(seed)
autoencoder_model %>%
keras::fit(features_matrix,
features_matrix,
epochs=epochs,
shuffle=TRUE,
verbose=0
)
encoder <- keras_model(inputs = autoencoder_model$input, outputs = encoder_layer)
encoded_data <- predict(encoder, features_matrix)
object@autoencoder <- encoded_data
print("Dimension reduction finished...")
return(object)
}
PreCluster <- function(object,res=NULL,seed){
img <- object@image
encoded_data <- object@autoencoder
rescale_to_range <- function(x,gmin,gmax){
rescaled_x <- (x-min(x))/(max(x)-min(x))*(gmax-gmin)+gmin
return(rescaled_x)
}
global_min <- mean(apply(encoded_data, MARGIN = 2, FUN = min))
global_max <- mean(apply(encoded_data, MARGIN = 2, FUN = max))
img <- img %>% mutate(RS_r=rescale_to_range(img$RS,global_min,global_max),
GS_r=rescale_to_range(img$GS,global_min,global_max),
BS_r=rescale_to_range(img$BS,global_min,global_max))
features <- cbind(encoded_data,img[,c("RS_r","GS_r","BS_r")])
# louvain clustering
set.seed(seed)
info.spatial = as.data.frame(features)
colnames(info.spatial) =  paste0("factor", 1:ncol(features))
knn.norm = FNN::get.knn(as.matrix(features), k = 100)
knn.norm = data.frame(from = rep(1:nrow(knn.norm$nn.index),
k=100), to = as.vector(knn.norm$nn.index), weight = 1/(1 + as.vector(knn.norm$nn.dist)))
nw.norm = igraph::graph_from_data_frame(knn.norm, directed = FALSE)
nw.norm = igraph::simplify(nw.norm)
if(is.null(res)){
lc.norm = igraph::cluster_louvain(nw.norm)
}else{
lc.norm = igraph::cluster_louvain(nw.norm,resolution = res)
}
print(paste("Found", length(unique(membership(lc.norm))), "clusters"))
img$cluster <- factor(lc.norm$membership)
object@pre_cluster <- factor(lc.norm$membership)
object@image <- img
# save clustering results
df <- data.frame(barcode=img$barcode, cluster=object@pre_cluster)
row.names(df) <- df$barcode
object@annotation <- df
write.table(df, file = "annotations.txt", quote = FALSE, col.names = FALSE, row.names = FALSE, sep = "\t")
return(object)
}
B1_obj2 <- reduceDim(object = B1_obj,seed=123)
devtools::load_all(".")
library(magick)
data("example_count")
data("example_loc")
image_path <- system.file("data", "example.jpg", package = "TUSCAN")
img <- image_read(image_path)
obj <- createTUSCANObject(counts = example_count, location = example_loc, img = img)
obj <- reduceDim(object = obj, seed = 1234)
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
devtools::load_all(".")
library(magick)
data("example_count")
data("example_loc")
image_path <- system.file("data", "example.jpg", package = "TUSCAN")
img <- image_read(image_path)
obj <- createTUSCANObject(counts = example_count, location = example_loc, img = img)
obj <- reduceDim(object = obj, seed = 1234)
library(reticulate)
py_config()
library(reticulate)
py_config()
py_config()
reticulate::py_config()
reticulate::py_install("tensorflow")
library(keras)
install_keras()
library(tensorflow)
install_tensorflow(envname = "r-tensorflow")
install_keras(tensorflow = "2.16.1")
library(tensorflow)
tf$constant("Hello TensorFlow!")
reticulate::py_config()
devtools::load_all(".")
library(magick)
data("example_count")
data("example_loc")
image_path <- system.file("data", "example.jpg", package = "TUSCAN")
img <- image_read(image_path)
obj <- createTUSCANObject(counts = example_count, location = example_loc, img = img)
obj <- reduceDim(object = obj, seed = 1234)
reticulate::py_last_error()
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
reticulate::py_last_error()
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
devtools::load_all(".")
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
library(magick)
data("example_count")
data("example_loc")
image_path <- system.file("data", "example.jpg", package = "TUSCAN")
img <- image_read(image_path)
obj <- createTUSCANObject(counts = example_count, location = example_loc, img = img)
obj <- reduceDim(object = obj, seed = 1234)
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
devtools::load_all(".")
obj <- reduceDim(object = obj, seed = 1234)
devtools::install_github("rstudio/keras")
devtools::load_all(".")
library(keras)
encoder <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(784)) %>%
layer_dense(units = 2, activation = 'relu')
reticulate::py_last_error()
encoder <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(784)) %>%
layer_dense(units = 2, activation = 'relu')
input_dim <- 2000
input_layer <- keras::layer_input(input_dim)
encoder_layer <- input_layer %>%
keras::layer_dense(units = 1500, activation = "relu", kernel_regularizer = regularizer_l1_l2(l1 = 1e-4)) %>%
keras::layer_batch_normalization() %>%
keras::layer_dense(units = 1000, activation = "relu", kernel_regularizer = regularizer_l1_l2(l1 = 1e-4)) %>%
keras::layer_dense(units = 500, activation = "relu") %>%
keras::layer_dense(units = 100, activation = "relu") %>%
keras::layer_dense(units = encoding_dim) # 2 dimensions for the output layer
encoding_dim <- 20
encoder_layer <- input_layer %>%
keras::layer_dense(units = 1500, activation = "relu", kernel_regularizer = regularizer_l1_l2(l1 = 1e-4)) %>%
keras::layer_batch_normalization() %>%
keras::layer_dense(units = 1000, activation = "relu", kernel_regularizer = regularizer_l1_l2(l1 = 1e-4)) %>%
keras::layer_dense(units = 500, activation = "relu") %>%
keras::layer_dense(units = 100, activation = "relu") %>%
keras::layer_dense(units = encoding_dim) # 2 dimensions for the output layer
decoder_layer <- encoder_layer %>%
keras::layer_dense(units = 100, activation = "relu") %>%
keras::layer_dense(units = 500, activation = "relu") %>%
keras::layer_dense(units = 1000, activation = "relu") %>%
keras::layer_dense(units = 1500, activation = "relu") %>%
keras::layer_dense(units = input_dim)
autoencoder_model <- keras::keras_model(inputs = input_layer, outputs = decoder_layer)
keras::keras_model(inputs = input_layer, outputs = decoder_layer)
View(decoder_layer)
View(encoder_layer)
autoencoder_model <- keras::keras_model(inputs = input_layer, outputs = decoder_layer)
autoencoder_model %>% keras::compile(
loss='mean_squared_error',
optimizer='adam',
metrics = c('accuracy')
)
devtools::load_all(".")
packageVersion("keras")
packageVersion("tensorflow")
install.packages("keras")
install.packages("keras")
packageVersion("tensorflow")
install.packages("keras")
packageVersion("keras")
library(keras)
install_keras()
devtools::load_all(".")
library(magick)
data("example_count")
data("example_loc")
image_path <- system.file("data", "example.jpg", package = "TUSCAN")
img <- image_read(image_path)
obj <- createTUSCANObject(counts = example_count, location = example_loc, img = img)
obj2 <- reduceDim(object = obj, seed = 1234)
obj3 <- PreCluster(object = obj, seed = 1234)
obj3 <- PreCluster(object = obj2, seed = 1234)
obj3 <- PreCluster(object = obj2, res=1.5,seed = 1234)
obj3 <- PreCluster(object = obj2, res=1.5,seed = 1234)
obj3 <- PreCluster(object = obj2, res=1.4, seed = 1234)
obj2 <- reduceDim(object = obj, seed = 1234)
obj3 <- PreCluster(object = obj2, seed = 1234)
obj3 <- PreCluster(object = obj2, seed = 1234)
obj3 <- PreCluster(object = obj2, res=1.4, seed = 1234)
devtools::load_all(".")
obj2 <- reduceDim(object = obj, seed = 1234)
at1 <- obj2@autoencoder
obj22 <- reduceDim(object = obj, seed = 1234)
at2 <- obj22@autoencoder
obj3 <- PreCluster(object = obj2, seed = 1234)
obj33 <- PreCluster(object = obj22, seed = 1234)
obj3 <- PreCluster(object = obj2, res = 1.5, seed = 1234)
obj33 <- PreCluster(object = obj22, res = 1.5, seed = 1234)
obj3 <- PreCluster(object = obj2, res = 1.3, seed = 1234)
obj33 <- PreCluster(object = obj22, res = 1.3, seed = 1234)
obj3 <- PreCluster(object = obj2, res = 1.4, seed = 1234)
obj33 <- PreCluster(object = obj22, res = 1.4, seed = 1234)
FindNormalCluster(obj3)
FindNormalCluster(obj3,w=0.3)
library(ggplot2)
df <- obj3@image
View(obj3)
ggplot(df, aes(x=x,y=y,color=as.factor(cluster)))+
geom_point(size=3)+
scale_y_reverse()+
scale_color_brewer(palette = "Set1")+
labs(title = "Pre-clustering result")+
theme_minimal()
cnv_obj_r1 <- runCNV(object = obj3, ref_group_names = c("1"), normalization = FALSE)
cnv_obj_r3 <- runCNV(object = obj3, ref_group_names = c("3"), normalization = FALSE)
plotCNVheatmap(object = cnv_obj_r1, title = "heatmap_r1")
plotCNVheatmap(object = cnv_obj_r3, title = "heatmap_r3")
tumor_r1 <- TumorCluster(object = cnv_obj_r1, seed = 1234)
tumor_r3 <- TumorCluster(object = cnv_obj_r3, seed = 1234)
df1 <- tumor_r1@image
p1 <- ggplot(df1, aes(x=x,y=y,color=as.factor(tumor)))+
geom_point(size=3)+
scale_y_reverse()+
scale_color_brewer(palette = "Set1")+
labs(title = "Tumor classification 1")+
theme_minimal()
df3 <- tumor_r3@image
p2 <- ggplot(df3, aes(x=x,y=y,color=as.factor(tumor)))+
geom_point(size=3)+
scale_y_reverse()+
scale_color_brewer(palette = "Set1")+
labs(title = "Tumor classification 3")+
theme_minimal()
library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
B1_ground_truth <- read.table("B1_truelabel.txt",header = TRUE)
B1_ground_truth <- read.table("B1_truelabel.txt",header = TRUE,sep="\t")
View(B1_ground_truth)
id <- df1$barcode
B1_ground_truth <- B1_ground_truth[match(id,B1_ground_truth$id),]
library(aricode)
ARI1 <- ARI(B1_ground_truth$tumor,df1$tumor)
ARI3 <- ARI(B1_ground_truth$tumor,df3$tumor)
obj4 <- PreCluster(object = obj2, res = 1.5, seed = 1234)
df <- obj4@image
ggplot(df, aes(x=x,y=y,color=as.factor(cluster)))+
geom_point(size=3)+
scale_y_reverse()+
scale_color_brewer(palette = "Set1")+
labs(title = "Pre-clustering result")+
theme_minimal()
table(df$cluster)
obj4 <- PreCluster(object = obj2, seed = 1234)
df <- obj4@image
table(df$cluster)
ggplot(df, aes(x=x,y=y,color=as.factor(cluster)))+
geom_point(size=3)+
scale_y_reverse()+
scale_color_brewer(palette = "Set1")+
labs(title = "Pre-clustering result")+
theme_minimal()
FindNormalCluster(obj4)
FindNormalCluster(obj4,w=0.3)
cnv_obj <- runCNV(object = obj4, ref_group_names = c("1"), normalization = FALSE)
tumor <- TumorCluster(object = cnv_obj, seed = 1234)
df4 <- tumor@image
p3 <- ggplot(df4, aes(x=x,y=y,color=as.factor(tumor)))+
geom_point(size=3)+
scale_y_reverse()+
scale_color_brewer(palette = "Set1")+
labs(title = "Tumor classification")+
theme_minimal()
grid.arrange(p1, p2,p3, ncol = 3)
use_r("plotSpatial")
library(devtools)
use_r("plotSpatial")
color <- RColorBrewer::brewer.pal(20, "Paired")
devtools::load_all(".")
plotSpatial(tumor,cluster = "cluster", size = 3)
devtools::load_all(".")
plotSpatial(tumor,cluster = "cluster", size = 3)
devtools::load_all(".")
plotSpatial(tumor,cluster = "cluster", size = 3)
devtools::load_all(".")
plotSpatial(tumor,cluster = "cluster", size = 3)
devtools::load_all(".")
plotSpatial(tumor,cluster = "tumor", size = 3)
devtools::load_all(".")
plotSpatial(tumor,class = "tumor", size = 3)
plotSpatial(tumor,class = "cluster", size = 3)
devtools::load_all(".")
plotSpatial(tumor,class = "cluster", size = 3)
devtools::load_all(".")
plotSpatial(tumor,class = "cluster", size = 3)
plotSpatial(tumor,class = "tumor", size = 3)
devtools::load_all(".")
tumor <- TumorCluster(object = cnv_obj, seed = 1234)
devtools::load_all(".")
obj <- createTUSCANObject(counts = example_count, location = example_loc, img = img)
obj2 <- reduceDim(object = obj, seed = 1234)
obj4 <- PreCluster(object = obj2, seed = 1234)
FindNormalCluster(obj4)
cnv_obj <- runCNV(object = obj4, ref_group_names = c("1"), normalization = FALSE)
tumor <- TumorCluster(object = cnv_obj, seed = 1234)
plotSpatial(tumor,class = "tumor", size = 3)
help(package="TUSCAN")
use_vignette("Intro_to_TUSCAN", title = "Introduction to TUSCAN")
help(package="TUSCAN")
help(runCNV)
help(package="CARD")
help(package="BayesSpace")
devtools::load_all(".")
use_r("TUSCAN")
help(package="TUSCAN")
help(package="BayesSpace")
help(package="CAMLU")
use_testthat()
library(TUSCAN)
devtools::load_all(".")
